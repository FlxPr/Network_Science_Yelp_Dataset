{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn.pytorch.conv import SAGEConv, GATConv\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from settings import file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to perform each step of training\n",
    "def train_step(g, features, edges, y, mask):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "    # Makes predictions\n",
    "    y_hat = model(g, features, edges)\n",
    "    # Computes loss\n",
    "    loss = loss_fn(y_hat[mask], y[mask])\n",
    "    # Computes gradients\n",
    "    loss.backward()\n",
    "    # Updates parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Returns the loss\n",
    "    return loss.item()\n",
    "\n",
    "# A helper function to perform each step of validation\n",
    "def val_step(g, features, edges, y, mask):\n",
    "    # Avoid to compute gradients\n",
    "    with th.no_grad():\n",
    "        # Switch to evaluation mode\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y_hat[mask], y[mask])\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "# Evaluate the accuracy on the test set\n",
    "def accuracy(model, g, features, edges, y, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges).argmax(dim=1).float()\n",
    "        y = y.float()\n",
    "            \n",
    "        # Calculate accuracies\n",
    "        errors = th.mean(th.abs(y[mask] - y_hat[mask]))\n",
    "            \n",
    "        print('Accuracy:', (1 - errors).item())\n",
    "        \n",
    "# Generate predictions\n",
    "def predict(model, g, features, edges):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges).argmax(dim=1).float()\n",
    "        \n",
    "        return y_hat.cpu().squeeze(0).detach().numpy() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_names['toronto_reviews_without_text'])\n",
    "df_users = pd.read_csv(file_names['toronto_users'])\n",
    "df_biz = pd.read_csv(file_names['toronto_businesses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Encode IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Add unique marks so that there wont be users and businesses that share the same ids\n",
    "df['user_id'] = df['user_id'] + 'U'\n",
    "df_users['user_id'] = df_users['user_id'] + 'U'\n",
    "df['business_id'] = df['business_id'] + 'B'\n",
    "df_biz['business_id'] = df_biz['business_id'] + 'B'\n",
    "\n",
    "# Fit the encoder\n",
    "le.fit(list(df['user_id'].unique()) + list(df['business_id'].unique()))\n",
    "\n",
    "# Encode the review table\n",
    "df['user_id'] = le.transform(df['user_id'])\n",
    "df['business_id'] = le.transform(df['business_id'])\n",
    "\n",
    "# Encode the business table\n",
    "df_biz = df_biz[df_biz['business_id'].isin(le.classes_)]\n",
    "df_biz['business_id'] = le.transform(df_biz['business_id'])\n",
    "\n",
    "# Encode, filter and transform the user table - only friendships between two toronto residents remain\n",
    "df_users['friends'] = df_users['friends'].str.split(', ')\n",
    "df_users = df_users.explode('friends')\n",
    "df_users['friends'] = df_users['friends'] + 'U'\n",
    "df_users = df_users[df_users['user_id'].isin(le.classes_)]\n",
    "df_users = df_users[df_users['friends'].isin(le.classes_)]\n",
    "df_users['user_id'] = le.transform(df_users['user_id'])\n",
    "df_users['friends'] = le.transform(df_users['friends'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Node Features - Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "cols = ['business_id', 'latitude', 'longitude', 'stars', 'review_count', 'attributes',\n",
    "       'categories']\n",
    "df_biz = df_biz[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the attributes column - Undone\n",
    "df_biz['attributes'] = df_biz['attributes'].map(eval, na_action='ignore')\n",
    "df_biz = df_biz.drop('attributes', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the categories column\n",
    "df_biz['categories'] = df_biz['categories'].str.split(', ')\n",
    "\n",
    "# Keep only categories that have at least 100 samples\n",
    "temp = df_biz.pop('categories').explode()\n",
    "temp = pd.crosstab(temp.index, temp)\n",
    "mask = temp.sum(axis=0)\n",
    "mask = (mask[mask>=100]).index\n",
    "df_biz = pd.concat([df_biz, temp[mask]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Aggregate user and business features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Part\n",
    "features = df_biz.rename(columns={'business_id':'id'})\n",
    "features['biz'] = True\n",
    "\n",
    "# User Part\n",
    "user_list = df_users['user_id'].append(df_users['friends']).unique()\n",
    "temp = pd.DataFrame({'id':user_list})\n",
    "temp['biz'] = False\n",
    "\n",
    "# Put everything together\n",
    "features = pd.concat([features, temp], axis=0)\n",
    "\n",
    "# Fill Nan values\n",
    "features = features.fillna(-999)\n",
    "\n",
    "# Sort the dataframe so that the row index corresponds to the index of the DGL graph\n",
    "features = features.sort_values('id')\n",
    "features = features.set_index('id')\n",
    "\n",
    "# Normalization\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "features = th.FloatTensor(features).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training data: 183052\n",
      "The size of validation data: 45764\n"
     ]
    }
   ],
   "source": [
    "# Separate chronically\n",
    "data = df.sort_values('date')\n",
    "edges = data.drop(['date', 'rating'], axis=1).values\n",
    "labels = data['rating'].values\n",
    "labels = th.LongTensor(labels).to(device) - 1 #IF I DONT REDUCE IT BY 1, PYTORCH WILL THINK I HAVE 6 CLASSES\n",
    "\n",
    "mask = np.arange(len(data))\n",
    "mask_train, mask_val = mask[:int(len(mask)*0.8)], mask[int(len(mask)*0.8):]\n",
    "\n",
    "print('The size of training data:', len(mask_train))\n",
    "print('The size of validation data:', len(mask_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An indirected user-business graph\n",
    "G = dgl.DGLGraph().to(device)\n",
    "G.add_nodes(len(le.classes_))\n",
    "G.add_edges(data['user_id'].to_list(), data['business_id'].to_list())\n",
    "\n",
    "# Add friend-friend edges\n",
    "G.add_edges(df_users['user_id'].to_list(), df_users['friends'].to_list())\n",
    "\n",
    "# Add edge attributes: -1 denotes a friend-friend relationship, 1-5 denote the rating of a user given to a business\n",
    "G.edata['y'] = np.array(list(data['rating'].values) + len(df_users['user_id']) * [-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GATConv(63, 100, 5, residual=True, activation=F.relu)\n",
    "        self.gcn2 = GATConv(100, 20, 3, residual=True, activation=F.relu)\n",
    "\n",
    "        self.fc1 = nn.Linear(40, 10)\n",
    "        self.fc2 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, g, features, edges):\n",
    "        \n",
    "        # Learning node embeddings\n",
    "        emb = self.gcn1(g, features)\n",
    "        emb = emb.max(1)[0]\n",
    "        emb = self.gcn2(g, emb)\n",
    "        emb = emb.max(1)[0]       \n",
    "        \n",
    "        # Encode nodes\n",
    "        emb1 = emb[edges[:, 0]]\n",
    "        emb2 = emb[edges[:, 1]]\n",
    "        emb_edges = th.cat([emb1, emb2], axis=1)\n",
    "        \n",
    "        # Classify edges\n",
    "        y = th.relu(self.fc1(emb_edges))\n",
    "        y = self.fc2(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = Net().to(device)\n",
    "\n",
    "# Loss function \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# The number of epochs\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.8795987367630005\n",
      "Epoch 0 Validation Loss: 1.7201324701309204\n",
      "Epoch 10 Training Loss: 1.5969792604446411\n",
      "Epoch 10 Validation Loss: 1.5950798988342285\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # 1 step of training\n",
    "    loss_train = train_step(G, features, edges, labels, mask_train)\n",
    "    losses_train.append(loss_train)\n",
    "    \n",
    "    # Keep track of validation loss\n",
    "    with th.no_grad():\n",
    "        # 1 step of validation\n",
    "        loss_val = val_step(G, features, edges, labels, mask_val)\n",
    "        losses_val.append(loss_val)\n",
    "    \n",
    "    # Report losses\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {} Training Loss: {}'.format(epoch, loss_train))\n",
    "        print('Epoch {} Validation Loss: {}'.format(epoch, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Training Phase--\n",
      "Accuracy: -0.0782017707824707\n",
      "--Validation Phase--\n",
      "Accuracy: -0.14625036716461182\n"
     ]
    }
   ],
   "source": [
    "print('--Training Phase--')\n",
    "accuracy(model, G, features, edges, labels, mask_train)\n",
    "print('--Validation Phase--')\n",
    "accuracy(model, G, features, edges, labels, mask_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
