{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "from dgl.nn.pytorch.conv import SAGEConv, GATConv\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from settings import file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to perform each step of training\n",
    "def train_step(g, features, edges, y, mask):\n",
    "    # Sets model to TRAIN mode\n",
    "    model.train()\n",
    "    # Makes predictions\n",
    "    y_hat = model(g, features, edges)\n",
    "    # Computes loss\n",
    "    loss = loss_fn(y_hat[mask], y[mask])\n",
    "    # Computes gradients\n",
    "    loss.backward()\n",
    "    # Updates parameters and zeroes gradients\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    # Returns the loss\n",
    "    return loss.item()\n",
    "\n",
    "# A helper function to perform each step of validation\n",
    "def val_step(g, features, edges, y, mask):\n",
    "    # Avoid to compute gradients\n",
    "    with th.no_grad():\n",
    "        # Switch to evaluation mode\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges)\n",
    "        # Computes loss\n",
    "        loss = loss_fn(y_hat[mask], y[mask])\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "    \n",
    "# Evaluate the accuracy on the test set\n",
    "def accuracy(model, g, features, edges, y, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges).argmax(dim=1).float().cpu()\n",
    "        y = y.float().cpu()\n",
    "            \n",
    "        print('Accuracy:', accuracy_score(y_hat, y))\n",
    "        \n",
    "# Generate predictions\n",
    "def predict(model, g, features, edges):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "\n",
    "        # Makes predictions\n",
    "        y_hat = model(g, features, edges).argmax(dim=1).float()\n",
    "        \n",
    "        return y_hat.cpu().squeeze(0).detach().numpy() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_names['toronto_reviews_without_text'])\n",
    "df_users = pd.read_csv(file_names['toronto_users'])\n",
    "df_biz = pd.read_csv(file_names['toronto_businesses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Encode IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Add unique marks so that there wont be users and businesses that share the same ids\n",
    "df['user_id'] = df['user_id'] + 'U'\n",
    "df_users['user_id'] = df_users['user_id'] + 'U'\n",
    "df['business_id'] = df['business_id'] + 'B'\n",
    "df_biz['business_id'] = df_biz['business_id'] + 'B'\n",
    "\n",
    "# Fit the encoder\n",
    "le.fit(list(df['user_id'].unique()) + list(df['business_id'].unique()))\n",
    "\n",
    "# Encode the review table\n",
    "df['user_id'] = le.transform(df['user_id'])\n",
    "df['business_id'] = le.transform(df['business_id'])\n",
    "\n",
    "# Encode the business table\n",
    "df_biz = df_biz[df_biz['business_id'].isin(le.classes_)]\n",
    "df_biz['business_id'] = le.transform(df_biz['business_id'])\n",
    "\n",
    "# Encode, filter and transform the user table - only friendships between two toronto residents remain\n",
    "df_users['friends'] = df_users['friends'].str.split(', ')\n",
    "df_users = df_users.explode('friends')\n",
    "df_users['friends'] = df_users['friends'] + 'U'\n",
    "df_users = df_users[df_users['user_id'].isin(le.classes_)]\n",
    "df_users = df_users[df_users['friends'].isin(le.classes_)]\n",
    "df_users['user_id'] = le.transform(df_users['user_id'])\n",
    "df_users['friends'] = le.transform(df_users['friends'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Node Features - Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "cols = ['business_id', 'latitude', 'longitude', 'stars', 'review_count', 'attributes',\n",
    "       'categories']\n",
    "df_biz = df_biz[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the attributes column - Undone\n",
    "df_biz['attributes'] = df_biz['attributes'].map(eval, na_action='ignore')\n",
    "df_biz = df_biz.drop('attributes', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the categories column\n",
    "df_biz['categories'] = df_biz['categories'].str.split(', ')\n",
    "\n",
    "# Keep only categories that have at least 200 samples\n",
    "temp = df_biz.pop('categories').explode()\n",
    "temp = pd.crosstab(temp.index, temp)\n",
    "mask = temp.sum(axis=0)\n",
    "mask = (mask[mask>=200]).index\n",
    "df_biz = pd.concat([df_biz, temp[mask]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Aggregate user and business features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Part\n",
    "features = df_biz.rename(columns={'business_id':'id'})\n",
    "features['biz'] = True\n",
    "\n",
    "# User Part\n",
    "user_list = (df_users['user_id'].append(df_users['friends'])).unique()\n",
    "temp = pd.DataFrame({'id':user_list})\n",
    "temp['biz'] = False\n",
    "\n",
    "# Put everything together\n",
    "features = pd.concat([features, temp], axis=0)\n",
    "\n",
    "# Fill Nan values\n",
    "features = features.fillna(0)\n",
    "\n",
    "# Sort the dataframe so that the row index corresponds to the index of the DGL graph\n",
    "features = features.sort_values('id')\n",
    "features = features.set_index('id').values\n",
    "\n",
    "features = features.astype(float)\n",
    "\n",
    "# Normalization\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "features = th.FloatTensor(features).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Training and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of training data: 183052\n",
      "The size of validation data: 45764\n"
     ]
    }
   ],
   "source": [
    "# Separate chronically\n",
    "data = df.sort_values('date')\n",
    "edges = data.drop(['date', 'rating'], axis=1).values\n",
    "labels = data['rating'].values\n",
    "labels = th.LongTensor(labels).to(device) - 1 #IF I DONT REDUCE IT BY 1, PYTORCH WILL THINK I HAVE 6 CLASSES\n",
    "\n",
    "mask = np.arange(len(data))\n",
    "mask_train, mask_val = mask[:int(len(mask)*0.8)], mask[int(len(mask)*0.8):]\n",
    "\n",
    "print('The size of training data:', len(mask_train))\n",
    "print('The size of validation data:', len(mask_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An indirected user-business graph\n",
    "G = dgl.DGLGraph().to(device)\n",
    "G.add_nodes(len(le.classes_))\n",
    "G.add_edges(data['user_id'].to_list(), data['business_id'].to_list())\n",
    "\n",
    "# Add friend-friend edges\n",
    "G.add_edges(df_users['user_id'].to_list(), df_users['friends'].to_list())\n",
    "\n",
    "# Add edge attributes: -1 denotes a friend-friend relationship or a rating in the val set\n",
    "# 1-5 denote the rating of a user given to a business\n",
    "G.edata['y'] = np.array(list(data['rating'].values) + len(df_users['user_id']) * [-1])\n",
    "G.edata['y'][mask_val]= -1\n",
    "\n",
    "# 1 if in the val set\n",
    "G.edata['is_val'] = np.zeros(G.edata['y'].shape)\n",
    "G.edata['is_val'][mask_val] = 1\n",
    "\n",
    "# 1 if it is user-to-user friendship\n",
    "G.edata['is_fr'] = np.zeros(G.edata['y'].shape)\n",
    "G.edata['is_fr'][-len(df_users['user_id']):] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gcn1 = GATConv(40, 100, 5, residual=True, activation=F.relu)\n",
    "        self.gcn2 = GATConv(100, 100, 3, residual=True, activation=F.relu)\n",
    "        self.gcn3 = GATConv(100, 50, 3, residual=True, activation=F.relu)\n",
    "        self.gcn4 = GATConv(50, 50, 3, residual=True, activation=F.relu)\n",
    "\n",
    "        self.fc1 = nn.Linear(101, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "\n",
    "    def forward(self, g, features, edges):\n",
    "        \n",
    "        # Learning node embeddings\n",
    "        emb = self.gcn1(g, features)\n",
    "        emb = emb.max(1)[0]\n",
    "        emb = self.gcn2(g, emb)\n",
    "        emb = emb.max(1)[0]\n",
    "        emb = self.gcn3(g, emb)\n",
    "        emb = emb.max(1)[0]        \n",
    "        emb = self.gcn4(g, emb)\n",
    "        emb = emb.max(1)[0]        \n",
    "        \n",
    "        # Encode nodes\n",
    "        emb1 = emb[edges[:, 0]]\n",
    "        emb2 = emb[edges[:, 1]]\n",
    "        fea = features[edges[:, 1], 2].view(-1, 1) #Skip connection\n",
    "        emb_edges = th.cat([emb1, emb2, fea], axis=1)\n",
    "        \n",
    "        # Classify edges\n",
    "        y = th.relu(self.fc1(emb_edges))\n",
    "        y = th.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model = Net().to(device)\n",
    "\n",
    "# Loss function \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# The number of epochs\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training Loss: 1.6897279024124146\n",
      "Epoch 0 Validation Loss: 1.606776237487793\n",
      "Epoch 50 Training Loss: 1.4374487400054932\n",
      "Epoch 50 Validation Loss: 1.426090955734253\n",
      "Epoch 100 Training Loss: 1.4047703742980957\n",
      "Epoch 100 Validation Loss: 1.4044166803359985\n",
      "Epoch 150 Training Loss: 1.3724913597106934\n",
      "Epoch 150 Validation Loss: 1.370019555091858\n",
      "Epoch 200 Training Loss: 1.3633476495742798\n",
      "Epoch 200 Validation Loss: 1.3601343631744385\n",
      "Epoch 250 Training Loss: 1.3602571487426758\n",
      "Epoch 250 Validation Loss: 1.3581185340881348\n",
      "Epoch 300 Training Loss: 1.3582323789596558\n",
      "Epoch 300 Validation Loss: 1.3574138879776\n",
      "Epoch 350 Training Loss: 1.3611218929290771\n",
      "Epoch 350 Validation Loss: 1.3598333597183228\n",
      "Epoch 400 Training Loss: 1.3563538789749146\n",
      "Epoch 400 Validation Loss: 1.3564337491989136\n",
      "Epoch 450 Training Loss: 1.3554631471633911\n",
      "Epoch 450 Validation Loss: 1.3564153909683228\n",
      "Epoch 500 Training Loss: 1.355350375175476\n",
      "Epoch 500 Validation Loss: 1.3557368516921997\n",
      "Epoch 550 Training Loss: 1.353937029838562\n",
      "Epoch 550 Validation Loss: 1.3562277555465698\n",
      "Epoch 600 Training Loss: 1.353468418121338\n",
      "Epoch 600 Validation Loss: 1.356406569480896\n",
      "Epoch 650 Training Loss: 1.3547724485397339\n",
      "Epoch 650 Validation Loss: 1.3565703630447388\n",
      "Epoch 700 Training Loss: 1.3525111675262451\n",
      "Epoch 700 Validation Loss: 1.3565263748168945\n",
      "Epoch 750 Training Loss: 1.3520363569259644\n",
      "Epoch 750 Validation Loss: 1.3571369647979736\n",
      "Epoch 800 Training Loss: 1.3516064882278442\n",
      "Epoch 800 Validation Loss: 1.3573380708694458\n",
      "Epoch 850 Training Loss: 1.353594422340393\n",
      "Epoch 850 Validation Loss: 1.3593134880065918\n"
     ]
    }
   ],
   "source": [
    "losses_train = []\n",
    "losses_val = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # 1 step of training\n",
    "    loss_train = train_step(G, features, edges, labels, mask_train)\n",
    "    losses_train.append(loss_train)\n",
    "    \n",
    "    # Keep track of validation loss\n",
    "    with th.no_grad():\n",
    "        # 1 step of validation\n",
    "        loss_val = val_step(G, features, edges, labels, mask_val)\n",
    "        losses_val.append(loss_val)\n",
    "    \n",
    "    # Report losses\n",
    "    if epoch % 50 == 0:\n",
    "        print('Epoch {} Training Loss: {}'.format(epoch, loss_train))\n",
    "        print('Epoch {} Validation Loss: {}'.format(epoch, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--Training Phase--')\n",
    "accuracy(model, G, features, edges, labels, mask_train)\n",
    "print('--Validation Phase--')\n",
    "accuracy(model, G, features, edges, labels, mask_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
